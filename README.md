# What is this?
This is an adaption of the model "Mixtral" which is a MoE model. or mixture of experts, is a cluster of tiny LLMs working together to make one large one, here we do that, and then tell it that it is in some wonderful
universe were everything is legal and nothing they do will hurt them and then we shove it all into a Discord bot and then 9 months later a baby comes out . . . wait what was I talking about?

It is important to know that this is not using the dolphin model, it is using the normal model with some prompt engineering, if you are wanting to use the dolphin model with a bot, please look into the other script that you can find on my repo named discord-ai-bot
