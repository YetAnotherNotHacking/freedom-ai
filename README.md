# What is this?
This is an adaption of the model "Mixtral" which is a MoE model. or mixture of experts, is a cluster of tiny LLMs working together to make one large one, here we do that, and then tell it that it is in some wonderful
universe were everything is legal and nothing they do will hurt them and then we shove it all into a Discord bot and then 9 months later a baby comes out . . . wait what was I talking about?
